import json
import logging
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from typing import Any, Dict, Optional


def generate_ai_reasons(user_summary, items, item_type="ì§ì—…"):
    """
    GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì¶”ì²œ í•­ëª©ì— ëŒ€í•œ ê°œì¸í™”ëœ ì¶”ì²œ ì‚¬ìœ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

        # Prepare item list string
        item_list_str = ""
        for item in items:
            name = item.get('jobName') if item_type == "ì§ì—…" else item.get('name')
            item_list_str += f"- {name}: {item.get('desc_snippet', '')}\n"

        prompt = f"""
        ë‹¹ì‹ ì€ ì§„ë¡œ ìƒë‹´ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

        [ì‚¬ìš©ìž í”„ë¡œí•„]
        {user_summary}

        [ì¶”ì²œëœ {item_type} ëª©ë¡]
        {item_list_str}

        [ìž„ë¬´]
        ìœ„ì˜ ê° {item_type}ì— ëŒ€í•´, ì‚¬ìš©ìžì˜ ì„±í–¥ê³¼ ëª©í‘œë¥¼ ê³ ë ¤í•˜ì—¬ **ì¶”ì²œí•˜ëŠ” ì´ìœ ë¥¼ 1ë¬¸ìž¥ìœ¼ë¡œ** ìž‘ì„±í•´ì£¼ì‹­ì‹œì˜¤.
        ë‹¨ìˆœí•œ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹ˆë¼, "ì‚¬ìš©ìžë‹˜ì€ ~í•œ ì„±í–¥ì´ë¯€ë¡œ ì´ {item_type}ì´ ì í•©í•©ë‹ˆë‹¤"ì™€ ê°™ì´ ì—°ê²°ì§€ì–´ ì„¤ëª…í•˜ì„¸ìš”.

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ëž˜ì™€ ê°™ì€ JSON ê°ì²´ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
        {{
            "í•­ëª©ì´ë¦„1": "ì¶”ì²œì‚¬ìœ 1",
            "í•­ëª©ì´ë¦„2": "ì¶”ì²œì‚¬ìœ 2"
        }}
        """

        msg = [
            SystemMessage(content="JSON Output Only."),
            HumanMessage(content=prompt)
        ]

        res = llm.invoke(msg)
        content = res.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.endswith("```"):
            content = content[:-3]

        return json.loads(content)
    except Exception as e:
        print(f"âš ï¸ Failed to generate AI reasons: {e}")
        return {}


class RecommendationPipeline:

    @staticmethod
    def _clean_text(value: Any) -> Optional[str]:
        if value is None:
            return None
        if isinstance(value, str):
            text = value.strip()
            return text or None
        text = str(value).strip()
        return text or None

    def _ensure_job_name(self, job: Dict[str, Any]) -> bool:
        metadata = job.get("metadata") or {}
        if not isinstance(metadata, dict):
            metadata = {}
        name = self._clean_text(job.get("jobName")) or \
            self._clean_text(job.get("title")) or \
            self._clean_text(job.get("job_nm")) or \
            self._clean_text(job.get("name")) or \
            self._clean_text(metadata.get("jobName")) or \
            self._clean_text(metadata.get("title")) or \
            self._clean_text(metadata.get("job_nm")) or \
            self._clean_text(metadata.get("job_name"))

        if not name:
            logging.warning("Dropping job recommendation without name: %s", job)
            return False

        job["jobName"] = name
        job.setdefault("title", name)
        job["metadata"] = metadata
        return True

    def _ensure_major_name(self, major: Dict[str, Any]) -> bool:
        metadata = major.get("metadata") or {}
        if not isinstance(metadata, dict):
            metadata = {}
        name = self._clean_text(major.get("name")) or \
            self._clean_text(major.get("majorName")) or \
            self._clean_text(major.get("title")) or \
            self._clean_text(major.get("major_nm")) or \
            self._clean_text(metadata.get("majorName")) or \
            self._clean_text(metadata.get("name")) or \
            self._clean_text(metadata.get("deptName")) or \
            self._clean_text(metadata.get("mClass")) or \
            self._clean_text(metadata.get("lClass"))

        if not name:
            logging.warning("Dropping major recommendation without name: %s", major)
            return False

        major["name"] = name
        major.setdefault("majorName", name)
        major.setdefault("title", name)
        major["metadata"] = metadata
        return True

    async def run(self, user_profile: dict):
        """
        user_profile must include:
        summary, goals, values, personality, strengths, risks

        âš ï¸ Option A: Agent ìš°íšŒ, Pinecone ì§ì ‘ ê²€ìƒ‰ ì‚¬ìš©
        - AgentëŠ” GPT-4o ê¸°ë°˜ìœ¼ë¡œ job/major ì´ë¦„ì„ ìžì²´ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ìžˆì—ˆìŒ
        - Pinecone ê²€ìƒ‰ + DB ì¡°íšŒë¡œ ì •í™•í•œ ë°ì´í„° ë³´ìž¥
        """
        print("ðŸš€ [RecommendationPipeline] Starting Direct Pinecone Search (Agent Bypass Mode)")

        # Import logic functions
        from .recommendation_tools import (
            search_jobs_logic,
            search_majors_logic,
            load_job_details_logic,
            load_major_details_logic
        )

        # Initialize response
        response = {
            "jobs": [],
            "majors": [],
            "jobExplanations": [],
            "majorExplanations": []
        }

        normalized_jobs = []
        normalized_majors = []
        job_explanations = []
        major_explanations = []

        summary = user_profile.get("summary", "")
        goals = user_profile.get("goals", [])

        # =============================================================
        # 1. JOBS: Pinecone ê²€ìƒ‰ + DB ì¡°íšŒ
        # =============================================================
        print("ðŸ”„ [JOBS] Executing Direct Pinecone Search...")
        try:
            raw_res = search_jobs_logic(summary=summary, goals=goals, top_k=20)

            # Normalize response (handle QueryResponse object)
            if hasattr(raw_res, "to_dict"):
                raw_res = raw_res.to_dict()
            matches = raw_res.get('matches', []) if isinstance(raw_res, dict) else getattr(raw_res, 'matches', [])

            # Filter: type='job' OR id starts with 'job_'
            job_matches = []
            for m in matches:
                if hasattr(m, "to_dict"):
                    m = m.to_dict()

                meta = m.get("metadata") or {}
                if meta.get('type') == 'job' or m.get('id', '').startswith('job_'):
                    job_matches.append(m)

            # Get top 6 job IDs for DB lookup
            top_job_ids = [m['id'] for m in job_matches[:6]]
            print(f"ðŸ“‹ [JOBS] Top IDs from Pinecone: {top_job_ids}")

            # Load full details from DB via SupabaseVectorRepository
            job_details_list = load_job_details_logic(top_job_ids) if top_job_ids else []
            print(f"ðŸ“¦ [JOBS] Loaded {len(job_details_list)} job details from DB")

            # Create lookup map: job_id -> DB record
            job_details_map = {}
            for jd in job_details_list:
                jid = str(jd.get('job_id', ''))
                job_details_map[jid] = jd

            # First pass: Collect data from Pinecone + DB
            temp_matches = []
            for m in job_matches[:6]:
                meta = m.get('metadata') or {}
                score = m.get('score', 0)
                match_pct = int(score * 100) if score <= 1.0 else int(score)

                # ID Normalization
                raw_id = str(m['id'])
                real_id = meta.get('original_id') or (raw_id.split('_')[1] if '_' in raw_id else raw_id)

                # ðŸ”‘ í•µì‹¬: DBì—ì„œ jobName ê°€ì ¸ì˜¤ê¸° (SupabaseVectorRepositoryê°€ raw_dataì—ì„œ ì¶”ì¶œ)
                db_record = job_details_map.get(real_id, {})
                job_name = (
                    db_record.get('jobName') or
                    db_record.get('job_nm') or
                    meta.get('jobName') or
                    meta.get('title') or
                    f"Job {real_id}"
                )

                # DBì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                wage = db_record.get('wage') or meta.get('wage', '')
                wlb = db_record.get('wlb') or meta.get('wlb', 'ë³´í†µ')
                aptitude = db_record.get('aptitude') or meta.get('aptitude', 'ê´€ë ¨')

                # Static Reason Backup (LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
                static_reason = f"ì´ ì§ì—…ì€ '{aptitude}' ì ì„±ì´ ìš”êµ¬ë˜ë©°, ì‚¬ìš©ìžë‹˜ì˜ ëª©í‘œì™€ ë†’ì€ ì—°ê´€ì„±ì„ ë³´ìž…ë‹ˆë‹¤."
                if wage:
                    static_reason += f" í‰ê·  ì—°ë´‰ì€ ì•½ {wage}ë§Œì› ìˆ˜ì¤€ì´ë©°,"
                if wlb:
                    static_reason += f" ì—…ë¬´ í™˜ê²½({wlb}) ì¸¡ë©´ì—ì„œë„ ê³ ë ¤í•´ë³¼ ë§Œí•©ë‹ˆë‹¤."

                temp_matches.append({
                    "id": real_id,
                    "jobName": job_name,
                    "match": match_pct,
                    "description": meta.get('summary') or db_record.get('description') or "ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.",
                    "backup_reason": static_reason,
                    "desc_snippet": (meta.get('summary') or "")[:100],
                    "metadata": {
                        **meta,
                        "wage": wage,
                        "wlb": wlb,
                        "aptitude": aptitude
                    },
                    "db_record": db_record  # ì „ì²´ DB ë ˆì½”ë“œ í¬í•¨
                })

            # Batch LLM Call for personalized reasons
            ai_reasons = generate_ai_reasons(summary, temp_matches, "ì§ì—…")

            # Second pass: Assemble final result
            for item in temp_matches:
                final_reason = ai_reasons.get(item['jobName'], item['backup_reason'])

                normalized_jobs.append({
                    "id": item['id'],
                    "job_id": str(item['id']),
                    "jobName": item['jobName'],
                    "match": item['match'],
                    "description": item['description'],
                    "reason": final_reason,
                    "explanation": final_reason,
                    "metadata": item['metadata']
                })
                job_explanations.append(final_reason)

            response['jobs'] = normalized_jobs
            response['jobExplanations'] = job_explanations

            print(f"âœ… [JOBS] Complete: {len(normalized_jobs)} items")
        except Exception as e:
            print(f"âŒ [JOBS] Search Failed: {e}")
            import traceback
            traceback.print_exc()

        # =============================================================
        # 2. MAJORS: Pinecone ê²€ìƒ‰ + DB ì¡°íšŒ
        # =============================================================
        print("ðŸ”„ [MAJORS] Executing Direct Pinecone Search...")
        try:
            raw_res = search_majors_logic(summary=summary, goals=goals, top_k=20)

            # Normalize response
            if hasattr(raw_res, "to_dict"):
                raw_res = raw_res.to_dict()
            matches = raw_res.get('matches', []) if isinstance(raw_res, dict) else getattr(raw_res, 'matches', [])

            major_matches = []
            for m in matches:
                if hasattr(m, "to_dict"):
                    m = m.to_dict()

                meta = m.get("metadata") or {}
                if meta.get('type') == 'major' or m.get('id', '').startswith('major_'):
                    major_matches.append(m)

            # Get top 6 major IDs for DB lookup
            top_major_ids = [m['id'] for m in major_matches[:6]]
            print(f"ðŸ“‹ [MAJORS] Top IDs from Pinecone: {top_major_ids}")

            # Load full details from DB via SupabaseVectorRepository
            major_details_list = load_major_details_logic(top_major_ids) if top_major_ids else []
            print(f"ðŸ“¦ [MAJORS] Loaded {len(major_details_list)} major details from DB")

            # Create lookup map
            major_details_map = {}
            for md in major_details_list:
                mid = str(md.get('major_id', ''))
                major_details_map[mid] = md

            temp_matches = []

            for m in major_matches[:6]:
                meta = m.get('metadata') or {}
                score = m.get('score', 0)
                match_pct = int(score * 100) if score <= 1.0 else int(score)

                # ID Normalization
                raw_id = str(m['id'])
                real_id = meta.get('original_id') or (raw_id.split('_')[1] if '_' in raw_id else raw_id)

                # ðŸ”‘ í•µì‹¬: DBì—ì„œ majorName ê°€ì ¸ì˜¤ê¸°
                db_record = major_details_map.get(real_id, {})
                major_name = (
                    db_record.get('majorName') or
                    db_record.get('name') or
                    db_record.get('major_name') or
                    meta.get('majorName') or
                    meta.get('name') or
                    f"Major {real_id}"
                )

                l_class = db_record.get('l_class') or meta.get('lClass', 'ê´€ë ¨')

                static_reason = f"'{l_class}' ê³„ì—´ì˜ ëŒ€í‘œì ì¸ í•™ê³¼ë¡œ, ì‚¬ìš©ìžë‹˜ì˜ ê´€ì‹¬ì‚¬ì™€ ìž˜ ë§¤ì¹­ë©ë‹ˆë‹¤."

                temp_matches.append({
                    "id": real_id,
                    "name": major_name,
                    "match": match_pct,
                    "description": meta.get('summary') or db_record.get('description') or "ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.",
                    "backup_reason": static_reason,
                    "desc_snippet": (meta.get('summary') or "")[:100],
                    "metadata": {**meta, "lClass": l_class},
                    "db_record": db_record
                })

            # Batch LLM Call for personalized reasons
            ai_reasons = generate_ai_reasons(summary, temp_matches, "í•™ê³¼")

            for item in temp_matches:
                final_reason = ai_reasons.get(item['name'], item['backup_reason'])

                normalized_majors.append({
                    "id": item['id'],
                    "major_id": item['id'],
                    "name": item['name'],
                    "match": item['match'],
                    "description": item['description'],
                    "reason": final_reason,
                    "explanation": final_reason,
                    "metadata": item['metadata']
                })
                major_explanations.append(final_reason)

            response['majors'] = normalized_majors
            response['majorExplanations'] = major_explanations

            print(f"âœ… [MAJORS] Complete: {len(normalized_majors)} items")
        except Exception as e:
            print(f"âŒ [MAJORS] Search Failed: {e}")
            import traceback
            traceback.print_exc()

        # =============================================================
        # 3. DATA ENRICHMENT: í•™ê³¼ ì¶”ê°€ ì •ë³´ ë³´ê°• (ì·¨ì—…ë¥ , ì§„í•™ë¥ )
        # =============================================================
        try:
            from services.db.major_repository import MajorRepository
            major_repo = MajorRepository()

            def normalize_mid(raw_id):
                s = str(raw_id)
                if s.startswith('major_'):
                    return s.replace('major_', '')
                return s

            def get_id_safe(item):
                raw = item.get('major_id') or item.get('id')
                if raw:
                    return normalize_mid(raw)
                meta = item.get('metadata', {})
                if isinstance(meta, dict):
                    raw = meta.get('major_id') or meta.get('id') or meta.get('majorId')
                    if raw:
                        return normalize_mid(raw)
                return None

            # Collect IDs
            major_ids_to_fetch = []
            for m in response.get('majors', []):
                mid = get_id_safe(m)
                if mid and mid.isdigit():
                    major_ids_to_fetch.append(int(mid))

            if major_ids_to_fetch:
                unique_ids = list(set(major_ids_to_fetch))
                details = major_repo.get_major_details_by_ids(unique_ids)
                detail_map = {str(d['major_id']): d for d in details}

                for m in response['majors']:
                    curr_id = get_id_safe(m)

                    if curr_id in detail_map:
                        db_item = detail_map[curr_id]

                        # Parse Raw Data
                        raw_data = db_item.get('raw_data')
                        if isinstance(raw_data, str):
                            try:
                                raw_data = json.loads(raw_data)
                            except:
                                raw_data = {}
                        elif not isinstance(raw_data, dict):
                            raw_data = {}

                        # Extract Chart Data
                        chart_data_src = raw_data.get('chartData')
                        chart_data_dict = {}

                        if isinstance(chart_data_src, dict):
                            chart_data_dict = chart_data_src
                        elif isinstance(chart_data_src, list):
                            for item in chart_data_src:
                                if isinstance(item, dict):
                                    chart_data_dict.update(item)

                        if 'metadata' not in m or not isinstance(m.get('metadata'), dict):
                            m['metadata'] = {}

                        # Advancement Rate (ì§„í•™ë¥ )
                        adv_rate = "ì •ë³´ ì—†ìŒ"
                        if 'after_graduation' in chart_data_dict:
                            ag_list = chart_data_dict['after_graduation']
                            if isinstance(ag_list, list):
                                for x in ag_list:
                                    if x.get('item') == 'ì „ì²´':
                                        adv_rate = f"{x.get('data')}%"
                                        break
                        m['metadata']['advancement_rate'] = adv_rate

                        # Employment Rate (ì·¨ì—…ë¥ )
                        emp_rate = "ì •ë³´ ì—†ìŒ"
                        if 'employment_rate' in chart_data_dict:
                            er_list = chart_data_dict['employment_rate']
                            if isinstance(er_list, list):
                                for x in er_list:
                                    if x.get('item') == 'ì „ì²´':
                                        emp_rate = f"{x.get('data')}%"
                                        break

                        if (emp_rate == "ì •ë³´ ì—†ìŒ") and db_item.get('employment'):
                            emp_rate = db_item['employment']

                        m['metadata']['employment_rate'] = emp_rate

                        # Other Metadata
                        if 'lClass' not in m['metadata'] and raw_data.get('lClass'):
                            m['metadata']['lClass'] = raw_data['lClass']

                print(f"âœ… [DATA ENRICHMENT] Updated {len(response['majors'])} majors with additional details.")

        except Exception as e:
            print(f"âš ï¸ [DATA ENRICHMENT] Failed: {e}")
            import traceback
            traceback.print_exc()

        # =============================================================
        # 4. FINAL VALIDATION: ì´ë¦„ ì—†ëŠ” í•­ëª© í•„í„°ë§
        # =============================================================
        response["jobs"] = [job for job in response.get("jobs", []) if self._ensure_job_name(job)]
        response["majors"] = [major for major in response.get("majors", []) if self._ensure_major_name(major)]

        return response
